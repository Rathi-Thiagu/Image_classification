{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLASSIFICATION OF IMAGES â€“INTEL DATASET Using MLP(Multi Layer Perceptron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing essential libraries\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os \n",
    "import cv2\n",
    "import random \n",
    "import math\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input the Directory path where the datasets(seg_train and seg_test) are stored\n",
    "DATADIRTRAIN =\"C:\\Datasets\\seg_train\" #path for seg_train\n",
    "DATADIRTEST=\"C:\\Datasets\\seg_test\"    #path for seg_test\n",
    "CATEGORIES= [\"street\",\"forest\",\"mountain\",\"sea\",\"glacier\",\"buildings\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing the data\n",
    "for category in CATEGORIES:\n",
    "    path =os.path.join(DATADIRTRAIN,category)\n",
    "    for img in os.listdir(path):\n",
    "        img_array = cv2.imread(os.path.join(path,img),cv2.IMREAD_COLOR)\n",
    "        plt.imshow(img_array)\n",
    "        plt.show()\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the training data\n",
    "training_data = []   \n",
    "IMG_SIZE = 100    # Resizing the Image to 100x100 pixels\n",
    "\n",
    "def create_training_data():\n",
    "    for category in CATEGORIES:\n",
    "        path =os.path.join(DATADIRTRAIN,category)\n",
    "        class_num= CATEGORIES.index(category)\n",
    "        for img in os.listdir(path):\n",
    "            try:\n",
    "                img_array = cv2.imread(os.path.join(path,img),cv2.IMREAD_GRAYSCALE) #Conversion to Grayscale\n",
    "                new_array = cv2.resize(img_array,(IMG_SIZE , IMG_SIZE)) \n",
    "                training_data.append([new_array,class_num])\n",
    "            except Exception as e:\n",
    "                pass\n",
    "            \n",
    "create_training_data()\n",
    "print(len(training_data))\n",
    "random.shuffle(training_data)  #Preprocessing Technique: Performing random shuffling of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating inputs and labels of training data\n",
    "X_train=[]\n",
    "y_train=[]\n",
    "\n",
    "for features, label in training_data:\n",
    "    X_train.append(features)\n",
    "    y_train.append(label)\n",
    "X_train=np.array(X_train)\n",
    "y_train=np.array(y_train)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing Technique: Reshaping Training dataset\n",
    "X_train = np.array(X_train).reshape(len(training_data), -1)\n",
    "print(X_train.shape)\n",
    "y_train = np.array(y_train).reshape(-1,1)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the testing data\n",
    "testing_data = []\n",
    "IMG_SIZE=100   #Preprocessing technique: Resizing the Image to 100x100 pixels\n",
    "\n",
    "def create_testing_data():\n",
    "    for category in CATEGORIES:\n",
    "        path =os.path.join(DATADIRTEST,category)\n",
    "        class_num= CATEGORIES.index(category)\n",
    "        for img in os.listdir(path):\n",
    "            try:\n",
    "                img_array = cv2.imread(os.path.join(path,img),cv2.IMREAD_GRAYSCALE) #Conversion to Grayscale\n",
    "                new_array = cv2.resize(img_array,(IMG_SIZE , IMG_SIZE)) \n",
    "                testing_data.append([new_array,class_num])\n",
    "            except Exception as e:\n",
    "                pass\n",
    "            \n",
    "create_testing_data()\n",
    "print(len(testing_data))\n",
    "random.shuffle(testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating inputs and labels of testing data\n",
    "X_test=[]\n",
    "y_test=[]\n",
    "\n",
    "for features, label in testing_data:\n",
    "    X_test.append(features)\n",
    "    y_test.append(label)\n",
    "X_test=np.array(X_test)\n",
    "y_test=np.array(y_test)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing Technique: Reshaping Testing dataset\n",
    "X_test= np.array(X_test).reshape(len(testing_data),-1)\n",
    "print(X_test.shape)\n",
    "y_test=np.array(y_test).reshape(-1,1)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross Validation Technique(Validation set Approach)\n",
    "X_train, X_val, y_train, y_val  = train_test_split(X_train, y_train, test_size = 0.2, random_state = 42)\n",
    "print(X_train.shape, X_val.shape)\n",
    "print(y_train.shape, y_val.shape)\n",
    "\n",
    "#Stratified Kfold\n",
    "#skf=StratifiedKFold(n_splits=6, shuffle=True, random_state=42)\n",
    "#skf.get_n_splits(X_train,y_train)\n",
    "#for train_index, Val_index in skf.split(X_train,y_train):\n",
    "#    print(\"TRAIN:\", train_index, \"TEST:\", Val_index)\n",
    "#    X_train_s, X_val_s = X_train[train_index], X_train[Val_index]\n",
    "#    y_train_s, y_val_s = y_train[train_index], y_train[Val_index]\n",
    "\n",
    "#Kfold\n",
    "#kf=KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "#kf.get_n_splits(X_train)\n",
    "#for train_index, Val_index in kf.split(X_train):\n",
    "#    print(\"TRAIN:\", train_index, \"TEST:\", Val_index)\n",
    "#    X_train_s, X_val_s = X_train[train_index], X_train[Val_index]\n",
    "#    y_train_s, y_val_s = y_train[train_index], y_train[Val_index]\n",
    "\n",
    "#LOOCV\n",
    "#kf=KFold(n_splits=X_train.shape[0], shuffle=True, random_state=42)\n",
    "#kf.get_n_splits(X_train)\n",
    "#for train_index, Val_index in kf.split(X_train):\n",
    "#    print(\"TRAIN:\", train_index, \"TEST:\", Val_index)\n",
    "#    X_train_s, X_val_s = X_train[train_index], X_train[Val_index]\n",
    "#    y_train_s, y_val_s = y_train[train_index], y_train[Val_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adaptive Learning Rate(ALR) Model Improvements\n",
    "#Step Decay of learning rate \n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "def step_decay(epoch):\n",
    "   initial_lrate = 0.1 #Setting the initial learning rate to 0.1 \n",
    "   drop = 0.5\n",
    "   epochs_drop = 10.0  #Drops the learning rate to half after every 10 epochs\n",
    "   lrate = initial_lrate * math.pow(drop,  \n",
    "           math.floor((1+epoch)/epochs_drop))\n",
    "   return lrate\n",
    "lrate = LearningRateScheduler(step_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Keras to build and train the neural network model\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "#model 10000-3000-2000-6\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(3000, activation =\"relu\"))\n",
    "model.add(keras.layers.Dense(2000, activation =\"relu\"))\n",
    "model.add(keras.layers.Dropout(0.5))   #Performing Dropout with 0.5 dropout rate\n",
    "model.add(keras.layers.Dense(6, activation = \"softmax\"))\n",
    "\n",
    "#model 10000-8700-8700-6\n",
    "#model = keras.models.Sequential()\n",
    "#model.add(keras.layers.Dense(8700, activation =\"relu\"))\n",
    "#model.add(keras.layers.Dense(8700, activation =\"relu\"))\n",
    "#model.add(keras.layers.Dropout(0.5))\n",
    "#model.add(keras.layers.Dense(6, activation = \"softmax\"))\n",
    "\n",
    "#model 10000-2000-300-300-128-6\n",
    "#model = keras.models.Sequential()\n",
    "#model.add(keras.layers.Dense(2000, activation =\"relu\"))\n",
    "#model.add(keras.layers.Dense(300, activation =\"relu\"))\n",
    "#model.add(keras.layers.Dropout(0.5))\n",
    "#model.add(keras.layers.Dense(300, activation =\"relu\"))\n",
    "#model.add(keras.layers.Dropout(0.5))\n",
    "#model.add(keras.layers.Dense(128, activation =\"relu\"))\n",
    "#model.add(keras.layers.Dropout(0.5))\n",
    "#model.add(keras.layers.Dense(6, activation = \"softmax\"))\n",
    "\n",
    "#Trying Different Activation Function in the hidden layer\n",
    "#model = keras.models.Sequential()\n",
    "#model.add(keras.layers.Dense(3000, activation =\"sigmoid\"))\n",
    "#model.add(keras.layers.Dense(2000, activation =\"sigmoid\"))\n",
    "#model.add(keras.layers.Dense(6, activation = \"softmax\"))\n",
    "\n",
    "#model = keras.models.Sequential()\n",
    "#model.add(keras.layers.Dense(3000, activation =\"relu\"))\n",
    "#model.add(keras.layers.Dense(2000, activation =\"relu\"))\n",
    "#model.add(keras.layers.Dense(6, activation = \"sigmoid\"))\n",
    "\n",
    "\n",
    "#Data scaling- for cross validation set approach\n",
    "X_train = X_train.astype('float32')/255.0\n",
    "y_train = y_train.astype('uint')\n",
    "X_val = X_val.astype('float32')/255.0\n",
    "y_val = y_val.astype('uint')\n",
    "\n",
    "#for Cross validation Kfold, skfold, LOOCV\n",
    "#X_train = X_train_s.astype('float32')/255.0\n",
    "#y_train = y_train_s.astype('uint')\n",
    "#X_val = X_val_s.astype('float32')/255.0\n",
    "#y_val = y_val_s.astype('uint')\n",
    "\n",
    "#Optimization Technique\n",
    "sgd = SGD(learning_rate=0.001)\n",
    "model.compile(optimizer=sgd,\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "#Regularisation Technique \n",
    "early_stopping_monitor = EarlyStopping(patience=5)\n",
    "\n",
    "#Tensorflow Log and Graph generator\n",
    "NAME =\"Project1-Model\"\n",
    "tensorboard = TensorBoard(log_dir='logs_1/{}'.format(NAME))\n",
    "\n",
    "#Defining callbacks with ALR, Early stopping , Tensorboard\n",
    "callbacks_list = [early_stopping_monitor,tensorboard,lrate]\n",
    "#callback with only Early stopping  \n",
    "#callbacks_list = [early_stopping_monitor]\n",
    "#callback with only lrate\n",
    "#callbacks_list = [lrate]\n",
    "#callback with only tensorboard\n",
    "#callbacks_list = [tensorboard]\n",
    "\n",
    "#Fit model using the training and validation dataset\n",
    "model.fit(X_train, y_train, batch_size=128, epochs=20, validation_data =(X_val,y_val), callbacks =callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datascaling of testing data\n",
    "X_test = X_test.astype('float32')/255.0\n",
    "y_test = y_test.astype('uint')\n",
    "\n",
    "#Run the unseen data using the predicted model to find the test data accuracy\n",
    "loss, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Accuracy: %.3f' % acc)\n",
    "\n",
    "#Calculating Classification Error \n",
    "err=1-acc\n",
    "print('Classification Error: %.3f' % err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluating Model Performance by creating confusion matrix and classification report\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "y_proba = model.predict(X_test) \n",
    "y_pred = model.predict_classes(X_test)\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(y_pred, y_test))\n",
    "print('Classification Report')\n",
    "print(classification_report(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the Confusion Matrix\n",
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True):\n",
    "    import itertools\n",
    "\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(confusion_matrix(y_pred, y_test), \n",
    "                      normalize    = False,\n",
    "                      target_names = [\"street\",\"forest\",\"mountain\",\"sea\",\"glacier\",\"buildings\"],\n",
    "                      title        = \"Confusion Matrix\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
